# -*- coding: utf-8 -*-
"""Scikit_Learn_Nb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-c0BWdvfDdhNQ6AD0UvAV8feEhwBZYKx

#Datasets
"""

from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

df = fetch_california_housing(as_frame=True).frame
df

import matplotlib.pyplot as plt

df.hist(figsize=(12,8))
plt.tight_layout()

from sklearn.datasets import fetch_openml, make_blobs, make_moons

X, y = make_moons(noise=0.1, random_state=42)

X

y

plt.scatter(X[:,0], X[:,1], c=y)

"""#Splitting Data"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

data = load_iris()
X, y = data.data, data.target

X, y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

X_train, y_train, X_test, y_test

import numpy as np
import matplotlib.pyplot as plt

counts = np.bincount(y_train)
positions = np.arange(len(counts))

plt.bar(positions, counts)
plt.xticks(positions, data.target_names)

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2)

for train_index, test_index in split.split(X, y):
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

"""#Preprocessing"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler

X, y = load_iris(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled

X_min = np.min(X_train, axis=0)
X_max = np.max(X_train, axis=0)

(X_train - X_min) / (X_max - X_min)

"""#Encoding"""

from sklearn.datasets import fetch_openml
from sklearn.preprocessing import OrdinalEncoder
import pandas as pd

data = pd.DataFrame({
    'buying': ['vhigh', 'high', 'med', 'low'],
    'maint': ['vhigh', 'high', 'med', 'low'],
    'doors': ['2', '3', '4', '5more']
})
data

columns_to_encode = ['buying', 'maint', 'doors']
encoder = OrdinalEncoder()
data[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])
data

from sklearn.preprocessing import OneHotEncoder
import pandas as pd

import pandas as pd
import numpy as np

# Create a sample DataFrame similar to the 'adult' dataset
data = {
    'age': np.random.randint(18, 90, size=100),
    'workclass': np.random.choice(['Private', 'Self-emp-not-inc', 'Local-gov', 'Federal-gov', 'State-gov', 'Without-pay', 'Never-worked'], size=100),
    'fnlwgt': np.random.randint(13769, 1484705, size=100),
    'education': np.random.choice(['Bachelors', 'Some-college', '11th', 'HS-grad', 'Prof-school', 'Assoc-acdm', 'Assoc-voc', '9th', '7th-8th', '12th', 'Masters', '1st-4th', '10th', 'Doctorate', '5th-6th', 'Preschool'], size=100),
    'education-num': np.random.randint(1, 16, size=100),
    'marital-status': np.random.choice(['Married-civ-spouse', 'Divorced', 'Never-married', 'Separated', 'Widowed', 'Married-spouse-absent', 'Married-AF-spouse'], size=100),
    'occupation': np.random.choice(['Tech-support', 'Craft-repair', 'Other-service', 'Sales', 'Exec-managerial', 'Prof-specialty', 'Handlers-cleaners', 'Machine-op-inspct', 'Adm-clerical', 'Farming-fishing', 'Transport-moving', 'Priv-house-serv', 'Protective-serv', 'Armed-Forces'], size=100),
    'relationship': np.random.choice(['Wife', 'Own-child', 'Husband', 'Not-in-family', 'Other-relative', 'Unmarried'], size=100),
    'race': np.random.choice(['White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other', 'Black'], size=100),
    'sex': np.random.choice(['Female', 'Male'], size=100),
    'capital-gain': np.random.randint(0, 99999, size=100),
    'capital-loss': np.random.randint(0, 4356, size=100),
    'hours-per-week': np.random.randint(1, 99, size=100),
    'native-country': np.random.choice(['United-States', 'Mexico', 'Philippines', 'Germany', 'Canada', 'Puerto-Rico', 'El-Salvador'], size=100),
    'income': np.random.choice(['<=50K', '>50K'], size=100)
}

df_adult_sample = pd.DataFrame(data)

# Display the first few rows of the sample DataFrame
df_adult_sample.head()

columns_to_encode = ['occupation','race']

encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_data = encoder.fit_transform(df_adult_sample[columns_to_encode])
new_cols = encoder.get_feature_names_out(columns_to_encode)

encoded_data

new_cols

df_encoded = pd.DataFrame(encoded_data, columns=new_cols, index=df_adult_sample.index)

data_final = pd.concat([
    df_adult_sample.drop(columns=columns_to_encode), df_encoded],
    axis=1
)

data_final

"""#Classification"""

from sklearn.datasets import load_breast_cancer, fetch_california_housing
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

X, y = load_breast_cancer(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

clf = KNeighborsClassifier()
clf.fit(X_train_scaled, y_train)

clf.score(X_test_scaled, y_test)

single_instance = X_test_scaled[0]

clf.predict([single_instance])

y_test[0]

clf.predict_proba([single_instance])

y_pred = clf.predict(X_test_scaled)
accuracy_score(y_test, y_pred)

"""#Dimentionality/Complexity Reduction using PCA"""

X, y = fetch_california_housing(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

pca = PCA(n_components=2)
X_train_reduced = pca.fit_transform(X_train)
X_test_reduced = pca.transform(X_test)

X_train_reduced.shape

X_train.shape

"""#Cross-Validation"""

from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_breast_cancer # Import the breast cancer dataset
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
import numpy as np # Import numpy for mean calculation

X, y = load_breast_cancer(return_X_y=True) # Load the breast cancer dataset
X_scaled = StandardScaler().fit_transform(X)
clf = KNeighborsClassifier()

scores = cross_val_score(clf, X_scaled, y, cv=5)
print(f"Cross-validation scores: {scores}")
print(f"Mean cross-validation score: {np.mean(scores)}")

scores

import numpy as np

np.mean(scores)

"""#HyperParameter Tuning"""

from sklearn.datasets import load_iris
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

params = {
      'n_estimators':[10, 50, 100],
      'max_depth':[None, 5, 10],
      'min_samples_split':[2, 5, 10]
}

clf = RandomForestClassifier(n_jobs=-1)

grid = GridSearchCV(clf, params, cv=3)

grid.fit(X_train, y_train)

grid.best_params_

best_clf = grid.best_estimator_

best_clf

best_clf.score(X_test, y_test)

"""#Pipeline"""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=10)),
    ('clf', RandomForestClassifier())
])

pipe.fit(X_train, y_train)

pipe.score(X_test, y_test)

