# -*- coding: utf-8 -*-
"""Supervised_ML_Nb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nhPKwARyycsutVsMs99jn5LHiMxAbUVC
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler

cols = ["fLength", "fWidth", "fSize", "fConc", "fConc1", "fAsym", "fM3Long", "fM3Trans", "fAlpha", "fDist", "class"]
df = pd.read_csv("magic04.data", names = cols)
df.head()

df['class'] = (df['class'] == 'g').astype(int)

df.head()

"""#Train, validation, test datasets"""

train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])

def scale_dataset(dataframe, oversample=False):
    x = dataframe[dataframe.columns[:-1]].values
    y = dataframe[dataframe.columns[-1]].values

    scaler = StandardScaler()
    x = scaler.fit_transform(x)

    if oversample:
        ros = RandomOverSampler()
        x, y = ros.fit_resample(x, y)

    data = np.hstack((x, np.reshape(y, (-1, 1))))

    return data, x, y

train, x_train, y_train = scale_dataset(train, oversample=True)
valid, x_valid, y_valid = scale_dataset(valid, oversample=False)
test, x_test, y_test = scale_dataset(test, oversample=False)

"""#**Classification**

# kNN
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

clf = KNeighborsClassifier(n_neighbors=5)
clf.fit(x_train, y_train)

clf.predict([x_valid[2]])

y_valid[2]

clf.score(x_valid, y_valid)

"""#Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

clf = GaussianNB()
clf.fit(x_train, y_train)

clf.predict([x_valid[0]])

y_valid[0]

clf.score(x_test, y_test)

"""#Logistic Regression"""

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(x_train, y_train)

clf.predict([x_valid[0]])

y_valid[0]

clf.score(x_test, y_test)

"""#SupportVectorMachines (SVM)"""

from sklearn.svm import SVC

clf = SVC()
clf.fit(x_train, y_train)

clf.score(x_test, y_test)

"""#**Classification with Neural Nets**"""

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

clf = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_valid, y_valid))

model.evaluate(x_test, y_test)

import matplotlib.pyplot as plt

def plot_accuracy(clf):
  plt.plot(clf.history['accuracy'], label='accuracy')
  plt.plot(clf.history['val_accuracy'], label='val_accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.grid(True)
  plt.show()

def plot_loss(clf):
  plt.plot(clf.history['loss'], label='loss')
  plt.plot(clf.history['val_loss'], label='val_loss')
  plt.xlabel('Epochs')
  plt.ylabel('Binary Cross Entropy')
  plt.legend()
  plt.grid(True)
  plt.show()

plot_accuracy(clf)
plot_loss(clf)

"""#**Regression**

#Linear Regression
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import StandardScaler
import copy
import seaborn as sns
import tensorflow as tf
from sklearn.linear_model import LinearRegression

cols = ["date", "bike_count", "Hour", "Temp", "Humidity", "wind_speed", "visibility", "dew_point", "solar_radiation", "rain", "snow", "seasons", "holiday", "functioning"]
df = pd.read_csv('SeoulBikeData - SeoulBikeData.csv.csv')
df.columns = cols
df = df.drop(['date', 'seasons', 'holiday'], axis=1)
df.head()

df['functioning'] = df['functioning'].apply(lambda x: 0 if x == 'Yes' else 1)
df = df[df['Hour'] == 12]
df = df.drop(['Hour'], axis=1)
df.head()

for label in df.columns[1:]:
  plt.scatter(df[label], df['bike_count'])
  plt.title(label)
  plt.ylabel("Bike Count at Noon")
  plt.xlabel(label)
  plt.show()

df = df.drop(['wind_speed', 'visibility', 'functioning'], axis=1)

df.head()

train, val, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])

def get_xy(dataframe, y_label, x_labels=None, oversample=True):
  dataframe = copy.deepcopy(dataframe)
  if x_labels is None:
    X = dataframe[[c for c in dataframe.columns if c != y_label]].values
  else:
    if len(x_labels) == 1:
      X = dataframe[x_labels[0]].values.reshape(-1, 1)
    else:
      X = dataframe[x_labels].values

  y = dataframe[y_label].values.reshape(-1, 1)

  data = np.hstack((X, y))

  return data, X, y

_, X_train_temp, y_train_temp = get_xy(train, "bike_count", x_labels=["Temp"])
_, X_val_temp, y_val_temp = get_xy(val, "bike_count", x_labels=["Temp"])
_, X_test_temp, y_test_temp = get_xy(test, "bike_count", x_labels=["Temp"])

temp_reg = LinearRegression()
temp_reg.fit(X_train_temp, y_train_temp)

temp_reg.score(X_test_temp, y_test_temp)

temp_reg.predict(X_test_temp[0].reshape(1, -1))

y_test_temp[0]

plt.scatter(X_train_temp, y_train_temp, color="skyblue", label="Data")
x = tf.linspace(-20, 40, 100)
plt.plot(x, temp_reg.predict(np.array(x).reshape(-1, 1)), color="orange", label="Fit", linewidth=3)
plt.legend()
plt.title("Temp vs Bike Count")
plt.ylabel("Bike Count")
plt.xlabel("Temp")
plt.show()

"""#Multiple Linear Regression"""

train, val, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])
_, X_train_all, y_train_all = get_xy(train, "bike_count", x_labels=df.columns[1:])
_, X_val_all, y_val_all = get_xy(val, "bike_count", x_labels=df.columns[1:])
_, X_test_all, y_test_all = get_xy(test, "bike_count", x_labels=df.columns[1:])

all_reg = LinearRegression()
all_reg.fit(X_train_all, y_train_all)

all_reg.score(X_test_all, y_test_all)

"""#**Regression with Neural Nets**"""

temp_normalizer = tf.keras.layers.Normalization(input_shape=(1,), axis=None)
temp_normalizer.adapt(X_train_temp.reshape(-1))

temp_nn_model = tf.keras.Sequential([
    temp_normalizer,
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1)
])

temp_nn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss='mean_squared_error')

clf = temp_nn_model.fit(X_train_temp, y_train_temp, verbose=0, epochs=1000, batch_size=32, validation_data=(X_val_temp, y_val_temp))

def plot_loss(clf):
  plt.plot(clf.history['loss'], label='loss')
  plt.plot(clf.history['val_loss'], label='val_loss')
  plt.xlabel('Epochs')
  plt.ylabel('MSE')
  plt.legend()
  plt.grid(True)
  plt.show()

plot_loss(clf)

temp_nn_model.evaluate(X_test_temp, y_test_temp)

plt.scatter(X_train_temp, y_train_temp, color="skyblue", label="Data")
x = tf.linspace(-20, 40, 100)
plt.plot(x, temp_nn_model.predict(np.array(x).reshape(-1, 1)), label='Fit', color="orange", linewidth=3)
plt.legend()
plt.title("Temp vs Bike Count")
plt.ylabel("Bike Count")
plt.xlabel("Temp")
plt.show()

"""# Regression with Neural Nets (Multiple Features)"""

all_normalizer = tf.keras.layers.Normalization(input_shape=(6,), axis=-1)
all_normalizer.adapt(X_train_all)

all_nn_model = tf.keras.Sequential([
    all_normalizer,
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1)
])
all_nn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')

clf_all = all_nn_model.fit(X_train_all, y_train_all, verbose=0, epochs=1000, batch_size=32, validation_data=(X_val_all, y_val_all))

all_nn_model.evaluate(X_test_all, y_test_all)

plot_loss(clf_all)

y_pred_lr = all_reg.predict(X_test_all)
y_pred_nn = all_nn_model.predict(X_test_all)

def MSE(y_pred, y_real):
  return np.square(y_pred - y_real).mean()

MSE(y_pred_lr, y_test_all)

MSE(y_pred_nn, y_test_all)

ax = plt.axes(aspect='equal')
plt.scatter(y_test_all, y_pred_lr, label='Linear Regression')
plt.scatter(y_test_all, y_pred_nn, label='Neural Nets')
plt.xlabel('True Values')
plt.ylabel('Predictions')
lims = [0, 1800]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims, c="red")
plt.legend()
plt.show()

